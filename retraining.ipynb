{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Fedorov_retraining_task.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "YrvPLW0ubNZ1"
      },
      "source": [
        "!pip install PyGithub\n",
        "!pip install markdown\n",
        "!pip install beautifulsoup4\n",
        "!pip install contractions\n",
        "!pip install gensim\n",
        "!pip install nltk\n",
        "\n",
        "import re, string, unicodedata\n",
        "\n",
        "import gensim.downloader as api\n",
        "from gensim.models.word2vec import Word2Vec\n",
        "\n",
        "from gensim.test.utils import get_tmpfile\n",
        "from gensim.models import KeyedVectors\n",
        "\n",
        "from github import Github\n",
        "from github import UnknownObjectException\n",
        "\n",
        "import contractions\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "from bs4 import BeautifulSoup\n",
        "from markdown import markdown\n",
        "from nltk import word_tokenize, sent_tokenize\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk import FreqDist"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9d6qvceE5um5"
      },
      "source": [
        "## Utility functions\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IJyGS8Py5zy-"
      },
      "source": [
        "### Noise Removal"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gqkSObWP5y_a"
      },
      "source": [
        "def remove_urls(text):\n",
        "    text = re.sub(r'(https|http)?:\\/\\/(\\w|\\.|\\/|\\?|\\=|\\&|\\%)*\\b', '', text, flags=re.MULTILINE)\n",
        "    return(text)\n",
        "\n",
        "def strip_html(text):\n",
        "    soup = BeautifulSoup(text, \"html.parser\")\n",
        "\n",
        "    return soup.get_text()\n",
        "\n",
        "def remove_between_square_brackets(text):\n",
        "    return re.sub('\\[[^]]*\\]', '', text)\n",
        "\n",
        "def denoise_text(text):\n",
        "    text = strip_html(text)\n",
        "    text = remove_between_square_brackets(text)\n",
        "    text = remove_urls(text)\n",
        "    return text\n",
        "\n",
        "def replace_contractions(text):\n",
        "    return contractions.fix(text)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DLKruY6N5_lX"
      },
      "source": [
        "### Normalization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L6KUJRUF6CPR"
      },
      "source": [
        "def remove_non_ascii(words):\n",
        "    new_words = []\n",
        "    for word in words:\n",
        "        new_word = unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
        "        new_words.append(new_word)\n",
        "    return new_words\n",
        "\n",
        "def to_lowercase(words):\n",
        "    new_words = []\n",
        "    for word in words:\n",
        "        new_word = word.lower()\n",
        "        new_words.append(new_word)\n",
        "    return new_words\n",
        "\n",
        "def remove_punctuation(words):\n",
        "    new_words = []\n",
        "    for word in words:\n",
        "        new_word = re.sub(r'[^\\w\\s]', '', word)\n",
        "        if new_word != '':\n",
        "            new_words.append(new_word)\n",
        "    return new_words\n",
        "\n",
        "def contains_number(value):\n",
        "    if True in [char.isdigit() for char in value]:\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "def delete_numbers(words):\n",
        "    new_words = []\n",
        "    for word in words:\n",
        "        if contains_number(word) == False:\n",
        "            new_words.append(word)\n",
        "    return new_words\n",
        "\n",
        "def contains_underscore(word):\n",
        "    for c in word:\n",
        "      if c == '_':\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "def remove_stopwords(words):\n",
        "    new_words = []\n",
        "    STOPWORDS = stopwords.words('english') + ['http', 'https']\n",
        "    for word in words:\n",
        "        if word not in STOPWORDS and contains_underscore(word) == False:\n",
        "            new_words.append(word)\n",
        "    return new_words\n",
        "\n",
        "def get_wordnet_pos(word):\n",
        "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
        "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
        "    tag_dict = {\"J\": wordnet.ADJ,\n",
        "                \"N\": wordnet.NOUN,\n",
        "                \"V\": wordnet.VERB,\n",
        "                \"R\": wordnet.ADV}\n",
        "\n",
        "    return tag_dict.get(tag, wordnet.NOUN)\n",
        "\n",
        "def lemmatize_words(words):\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    lemmas = []\n",
        "    for word in words:\n",
        "        lemma = lemmatizer.lemmatize(word, get_wordnet_pos(word))\n",
        "        lemmas.append(lemma)\n",
        "    return lemmas\n",
        "\n",
        "def remove_short_and_long_words(words):\n",
        "    new_words = []\n",
        "    for word in words:\n",
        "        if len(word) > 1 and len(word) < 28:\n",
        "            new_words.append(word)\n",
        "    return new_words\n",
        "\n",
        "def normalize(words):\n",
        "    words = remove_non_ascii(words)\n",
        "    words = to_lowercase(words)\n",
        "    words = remove_punctuation(words)\n",
        "    words = delete_numbers(words)\n",
        "    words = remove_stopwords(words)\n",
        "    words = remove_short_and_long_words(words)\n",
        "    return words"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "APmX1jA2SYi_"
      },
      "source": [
        "def get_words_from_readme(text):\n",
        "    is_html = bool(BeautifulSoup(text, \"html.parser\").find())\n",
        "    text = re.sub(r\"```[^\\S\\r\\n]*[a-z]*\\n.*?\\n```\", '', text, 0, re.DOTALL)\n",
        "    text = strip_html(text)\n",
        "    html = text\n",
        "    if is_html == False:\n",
        "      html = markdown(text)\n",
        "\n",
        "    # Noise Removal\n",
        "    readme_text = denoise_text(html)\n",
        "    readme_text = replace_contractions(readme_text)\n",
        "\n",
        "    # Tokenization\n",
        "    words = nltk.word_tokenize(readme_text)\n",
        "    \n",
        "    # Normalization \n",
        "    words = normalize(words)\n",
        "    words = lemmatize_words(words)\n",
        "    return words\n",
        "\n",
        "def print_synonyms(word_vectors, most_common_words):\n",
        "    for word in most_common_words:\n",
        "        print(word + \" : \", end=\"\")\n",
        "        [print(synonym, end=' | ') for synonym, freq in word_vectors.most_similar(word, topn=5)]\n",
        "        print()"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KzhYZx0B9qfE"
      },
      "source": [
        "## Load data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2EqemHYtiq4g"
      },
      "source": [
        "org_name = 'apache'\n",
        "TOKEN = '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'\n",
        "\n",
        "g = Github(login_or_token=TOKEN)\n",
        "\n",
        "readme_files_content_md = []\n",
        "for repo in g.get_organization(org_name).get_repos():\n",
        "  if repo.fork == False:\n",
        "      try:\n",
        "          readme_file = repo.get_readme()\n",
        "          content_md = readme_file.decoded_content.decode('utf_8')\n",
        "          readme_files_content_md.append(content_md)\n",
        "      except UnknownObjectException:\n",
        "          continue\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aurpwRZq-_xG"
      },
      "source": [
        "## Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ukXunyuGVFWn"
      },
      "source": [
        "data = []\n",
        "for content in readme_files_content_md:\n",
        "    data.append(get_words_from_readme(content))"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iuS6YfbZEtRj"
      },
      "source": [
        "## Initialize model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2QQx6HPBEswn"
      },
      "source": [
        "model = Word2Vec(size=300, min_count=5)\n",
        "model.build_vocab(data)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KtK7upu51eiX"
      },
      "source": [
        "pretrained = api.load(\"glove-wiki-gigaword-300\")\n",
        "tmp_file = get_tmpfile(\"pretrained_vectors.txt\")\n",
        "pretrained.save_word2vec_format(tmp_file)\n",
        "init_vocab = [list(pretrained.vocab.keys())]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4wxB9HQ4FRlw"
      },
      "source": [
        "min_count = model.vocabulary.min_count\n",
        "model.vocabulary.min_count = 1\n",
        "model.build_vocab(init_vocab, update=True)\n",
        "model.intersect_word2vec_format(tmp_file, binary=False, lockf=1.0)\n",
        "model.vocabulary.min_count = min_count"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sC8NVkrF4FAN"
      },
      "source": [
        "## Synonyms before retraining"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tx8ioRtr1D9z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c27f570a-813a-4d9e-81f6-bd57eb55c25b"
      },
      "source": [
        "most_common_words = ['apache', 'function', 'docker', 'sling', \n",
        "                     'application', 'library', 'log',\n",
        "                     'framework', 'build', 'maven', 'request', 'branch', \n",
        "                     'script', 'jira']\n",
        "d = FreqDist()\n",
        "for docs in data:\n",
        "    d.update(FreqDist(docs))\n",
        "\n",
        "[\"Word = '{}' Frequency = {}\".format(word, d[word]) for word in most_common_words]"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"Word = 'apache' Frequency = 5789\",\n",
              " \"Word = 'function' Frequency = 933\",\n",
              " \"Word = 'docker' Frequency = 759\",\n",
              " \"Word = 'sling' Frequency = 1045\",\n",
              " \"Word = 'application' Frequency = 1204\",\n",
              " \"Word = 'library' Frequency = 1183\",\n",
              " \"Word = 'log' Frequency = 573\",\n",
              " \"Word = 'framework' Frequency = 481\",\n",
              " \"Word = 'build' Frequency = 3091\",\n",
              " \"Word = 'maven' Frequency = 1357\",\n",
              " \"Word = 'request' Frequency = 1205\",\n",
              " \"Word = 'branch' Frequency = 1054\",\n",
              " \"Word = 'script' Frequency = 806\",\n",
              " \"Word = 'jira' Frequency = 553\"]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wzWbxUso8YOn",
        "outputId": "0f0e3856-ac5e-4f3c-bc7f-45a9e3c02ca0"
      },
      "source": [
        "print_synonyms(pretrained, most_common_words)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "apache : apaches | ah-64 | kiowa | comanche | helicopter | \n",
            "function : functions | i.e. | functional | hence | defined | \n",
            "docker : oyapock | heatherington | dacre | guiot | puleston | \n",
            "sling : blade | swivels | slings | swivel | strap | \n",
            "application : applications | applied | apply | applying | user | \n",
            "library : libraries | archives | archive | museum | collections | \n",
            "log : logs | cabins | cabin | wooden | one-room | \n",
            "framework : implementation | frameworks | principles | implement | implementing | \n",
            "build : construct | develop | built | create | building | \n",
            "maven : mavens | doyenne | etiquette | homemaking | lingo | \n",
            "request : requested | requests | requesting | asked | asking | \n",
            "branch : branches | railway | established | line | offices | \n",
            "script : scripts | screenplay | written | writing | screenwriter | \n",
            "jira : producer-director | supernature | flabellina | 5l | selles | \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Q9skiDcE8oz"
      },
      "source": [
        "## Retraining"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J-Guhix2HUj7"
      },
      "source": [
        "model.train(data, total_examples=len(data), epochs=50)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E6BGWYwh4h2O"
      },
      "source": [
        "## Synonyms after retraining"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xrhS8MAs5nUo",
        "outputId": "ad086064-4614-4dca-cedd-43604f1975d9"
      },
      "source": [
        "print_synonyms(model.wv, most_common_words)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "apache : use | allow | us | component | build | \n",
            "function : functions | method | return | refaddeventlistener | define | \n",
            "docker : dockercompose | container | dockerfile | dockerhub | image | \n",
            "sling : branchtrunk | launchpad | uimaas | orgapacheslingsuperimposing | jcr | \n",
            "application : framework | use | app | project | apps | \n",
            "library : libraries | build | include | define | addition | \n",
            "log : logs | info | trace | logger | statistic | \n",
            "framework : application | different | business | solution | together | \n",
            "build : built | run | building | use | test | \n",
            "maven : gradle | mvn | pullplugin | artifact | version | \n",
            "request : requests | response | could | respond | rejected | \n",
            "branch : asfsite | merge | graysvg | asfstaging | the | \n",
            "script : scripts | screenplay | file | process | build | \n",
            "jira : ticket | componentsgit | expiretime | submit | pluginid | \n"
          ]
        }
      ]
    }
  ]
}